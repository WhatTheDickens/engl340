---
title: What is Artificial Intelligence?
layout: default
parent: Artificial Intelligence
nav_order: 2
---

# What is Artificial Intelligence?

Artificial intelligence&mdash;AI for short&mdash;especially a variety of that species known as generative artificial intelligence&mdash;GenAI&mdash;has been the talk of computer scientists, journalists, educators, economists, politicians, business executives, futurists, bloggers, and social media influencers&mdash;a partial list&mdash;ever since the company OpenAI released its chatbot ChatGPT in late 2022. <!-- (The "GPT" in ChatGPT stands for "generative pretrained transformer.) -->

But in fact AI is an old idea.

The article "[Artificial Intelligence](https://plato.stanford.edu/entries/artificial-intelligence/#HistAI)" in the *Stanford Encyclopedia of Philosophy* traces the roots of AI to Aristotle (who can be "credited with devising the first knowledge-bases and ontologies") and explains that the seventeenth-century French philosopher Descartes considered whether it would be possible to distinguish machines capable of speech and action from human beings. (His answer was yes.)

At the dawn of the modern computer age, the mathematician Alan Turing, who, [as we saw earlier]({{ site.url }}/mod-1.what), described the idea of a universal computer, also described, in a famous paper published in 1950, what has since come to be known as the "Turing Test"&mdash;a test, not of whether a machine could think, but rather of whether a machine could simulate the appearance of thought sufficiently to confound a human interlocutor.

The term "artificial intelligence," at least as it applies to computing, was born just a few years later, in a [proposal for a summer conference at Dartmouth College](https://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html) that took place in 1956. In their proposal, John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon called for a two-month study "to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer."

A key word in this quotation is *conjecture*. In the years since the 1956 Dartmouth conference, artificial intelligence has grown into a vibrant research field, one that has had ups and downs but that has produced remarkable breakthroughs, especially in this century, in the ability of computers to simulate speech and writing and to automate tasks. Nevertheless, whether "every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it" is a question on which there's no more agreement now than there was at the time of the conference. 

Moreover, whether a machine that can *simulate* intelligence can be said to *possess* intelligence&mdash;even of an "artificial" kind&mdash;isn't at all clear. The Dartmouth proposal glides smoothly from speculation about simulating intelligence to assertions about how a "truly intelligent machine" would behave. Some critics of the research field that emerged from the 1956 conference have focused on what they see as its too easy conflation of thinking with behavior that *looks like* that of thinking human beings, as though the difference didn't matter, or, worse, as though there is in fact no difference at all.

We'll look at this and other criticisms of AI in a later section of this module. For now, we should simply note that any attempt to define "artificial intelligence" in computing takes us into territory that's highly contested. The term doesn't pick out a single, clear, obviously achievable goal or set of computing methodologies. It may be best to keep those quotation marks around it and simply treat it as an imprecise but, whether we like it or not, increasingly entrenched catch-all label for a range of rapidly innovations in computer software loosely connected by some common features. The next page will examine some of these features and introduce you to additional terms you should know. 

<!-- One of the most famous of these criticisms, formulated in 1980, and described in detail in the Stanford Encyclopedia article mentioned above, was offered in a thought experiment by the philosopher John Searle. In the experiment, a human translator translates into English questions written in a language whose characters are unrecognizable to him; he produces his translations by using a simple look-up table. Even if his translations were perfect, Searle argues, it would make no sense to say that the translator "understood" the language he was translating.

A number of more recent critics have pointed to the irreducibly embodied and situational nature of human communication, in which intention, meaning-making, and striving play roles that are absent from the way "artificially intelligent" systems&mdash;even those that improve themselves by means of so-called "machine learning"&mdash;generate language. One of these, the computational linguist Emily Bender, profiled in a 2023 *New York* magazine article titled "[You Are Not a Parrot](https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html), points out that language "is built on 'people speaking to each other, working together to achieve a joint understanding. It's a human-human interaction.'" The title of the 2021 paper that Bender co-authored with three others (two of them members of Google's "Ethical AI" team who lost their jobs as a result), "[On the Dangers of Stochastic Parrots: Can Large Language Models Be Too Big?](https://dl.acm.org/doi/10.1145/3442188.3445922)", in addition to being the source of the *New York* profile's title, has turned the phrase "stochastic parrot" into a widely used shorthand for language-generating AI systems operate: not by striving to put thought into words but by a process of statistical inference in which they replicate patterns in the huge quantities of text on which they've been trained.

Similarly, the philosopher Alva Noë, in his essay "[Rage Against the Machine](https://aeon.co/essays/can-computers-think-no-they-cant-actually-do-anything)" has pointed out that human language-generation, unlike that of computers, involves elements of "resistance," "irritation," and "negotation"&mdash;negotiation not only with other speakers but with the meanings of words themselves, resulting, at times, in new or altered meanings, which in turn alter the speakers themselves. 

> We don’t just talk, as it were, following the rules blindly. Talking is an issue for us, and the rules, such as they are, are up for grabs and in dispute. We always, inevitably, and from the beginning, are made to cope with how hard talking is, how liable we are to misunderstand each other, although most of the time this is undertaken matter-of-factly and without undue stress. To talk, almost inevitably, is to question word choice, to demand reformulation, repetition and repair. What do you mean? How can you say that? In this way, talking contains within it, from the start, and as one of its basic modes, the activities of criticism and reflection about talking, which end up changing the way we talk. We don’t just act, as it were, in the flow. Flow eludes us and, in its place, we know striving, argument and negotiation. And so we change language in using language; and that’s what a language is, a place of capture and release, engagement and criticism, a process. We can never factor out mere doing, skilfulness, habit – the sort of things machines are used effectively to simulate – from the ways these doings, engagements and skills are made new, transformed, through our very acts of doing them. These are entangled. This is a crucial lesson about the very shape of human cognition. -->

<!-- ## The Selling of Artificial Intelligence 

Should this be its own page? Should it go on the "why controversial?" page

-->

<!-- To suppose that we can talk meaningfully about something called "artificial intelligence" is to assume that we can take, as our starting point, something we already understand, called "intelligence," and ask whether it's possible, or what it would look like, to replicate it "artificially."

But far from understanding intelligence, we don't even have an agreed-upon definition of it&mdash;any more than we have agreed-upon definitions of such related words as "thinking," "mind," and "consciousness." -->

<!-- This isn't to say there aren't theories about all of these or that there hasn't been progress in understanding them. -->

<!-- What we do have, accompanied by both extravagant claims for their efficacy and vehement calls to resist or severely regulate them&mdash;sometimes issuing from the same source&mdash;are computer systems and interfaces *presented to us* as exhibiting or employing artificial intelligence (AI for short). Sometimes these systems and interfaces are said to exhibit or employ a specific variety of artificial intelligence known as "generative artificial intelligence"&mdash;GenAI, for short.

AI and GenAI are thus brute, increasingly inescapable facts of current social reality at the same time that "AI" and "GenAI" are terms without clear, or perhaps any, conceptual content. This may be part of what authors Emily M. Bender and Alex Hanna have in mind when they write, in their book *The AI Con: How to Fight Big Tech's Hype and Create the Future We Want*,

> To put it bluntly, "AI" is a marketing term. It doesn't refer to a coherent set of technologies. Instead, the phrase "artificial intelligence" is deployed when the people building or selling a particular set of technologies will profit from getting others to believe that their technology is similar to humans, able to do things that, in fact, intrinsically require human judgment, perception, or creativity. --> 

<!-- p. 38 -->

<!-- This isn't to say that the technologies lumped together and marketed as AI aren't, individually, powerful and potentially valuable. The varieties of what Bender and Hanna prefer to call "automation"&mdash;such as automated decision-making, classification, recommendation, transcription/translation, and text/image generation systems&mdash;all have their uses, though their usefulness must be balanced against their proven and potential harms, discussed in more detail later in this module. Moreover, the research that makes these systems possible, in areas such as machine learning, natural language processing, and statistical inference, is rigorous and well defined.

What's undeniable, however, is that a critical understanding of these systems, and the research behind them, requires that we consider the impact of treating them as though they're approaching, or have already crossed, the line between machine and human. And it requires that we consider, as well, whose interests are served by presenting them in that light. -->

<!-- Eliza -->

<!-- Although there's no universally accepted definition of "artificial intelligence" in computing, the term is commonly used to describe one or more of the following:

- an area of research within computer science
- a property that some researchers claim (controversially) belongs or could belong to certain computer systems
- a set of technologies for generating new content (text, images, sound, etc.) from existing content

The last item on that list has given rise to the term "generative artificial intelligence" or, for short, "GenAI." While GenAI isn't new, it suddenly seems to be everywhere: not only in free-standing tools with names such as ChatGPT, Claude, Gemini, Copilot, DALL-E, and Midjourney, but also, increasingly, as an affordance built into other tools, such as search engines, word-processors, image editors, email and messaging interfaces, mobile apps, coding environments, websites, cars, and home appliances.

In this module, we'll make some use of GenAI and consider some of the ethical concerns currently being raised around its impact on society and the environment. 

But first, it will be helpful to put GenAI and artificial intelligence (AI) as a whole in some historical perspective. Data scientist William J.B. Mattingly's series of videos on AI and machine learning is a good place to start. Watch the first video in the series, below, to get a brief introduction to how artificial intelligence fits into the larger history of computing.

<iframe width="560" height="315" src="https://www.youtube.com/embed/G6cW5JybUPU?si=Ky9-pDmMc_9zqtmv" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## A few key terms

The definitions below have been adapted from José Antonio Bowen and C. Edward Watson, *Teaching with AI: A Practical Guide to a New Era of Human Learning* (John Hopkins University Press, 2024) and *Intro to AI*, a newsletter distributed by [*MIT Technology Review*](https://www.technologyreview.com/).

- **Artificial Intelligence (AI):** Depending on context, this term usually refers to an area of research within computer science, a property that some researchers claim (controversially) belongs or could belong to certain computer systems, or a set of technologies for generating new content (text, images, sound, etc.) This isn't an exhaustive list.
- **Expert Systems:** rules and logic programmed into a computer to anticipate a wide range of possible scenarios.
- **Machine Learning:** The use of probability and statistics to find patterns in huge quantities of data and generalize from these. Recommendation algorithms such as those used by Netflix, YouTube, and Spotify rely on machine learning, as do search engines, social media feeds, and voice assistants like Siri and Alexa.
- **Deep Learning:** Super-powered machine learning that uses massive datasets and large neural networks, and that requires a great deal of computing power (sometimes called "compute.")
- **Neural Networks:** Computing systems designed to emulate the neural connections in the human brain. The "neurons" in the network are connected by complex mathematical equations. When a new piece of data (a new image, for example) is passed through a neural network, it moves through the layers of the network and outputs a result. A **trained** neural network, aka a **model**, has learned through trial and error to reproduce patterns in its training data and produce (hopefully) correct answers.
- **Foundational Models:** Neural networks trained with a large data set using machine learning techniques that mimic human trial and error.
- **Large Language Models (LLMs):** Foundational models focused on language. Built on deep-learning algorithms that are trained on enormous quantities of text, they're able to predict which words are most likely to appear together in a sentence or paragraph and generate textual content that sounds or reads like what a person might say or write.
- **GPT:** Generative Pre-trained Transformers. Foundational models and LLMs all use GPT architecture.
- **Parameter:** an internal variable in a neural network that can be tuned or adjusted to change the output.
- **Diffusion Model** A type of foundational model used to create images and video.
- **Alignment:** The work that humans do to ensure that an AI system does only what users want it to do.

-->